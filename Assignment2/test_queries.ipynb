{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "punctuations = ['.', ',', '!', '\\'', '\\\"',\n",
    "                '(', ')', '[', ']', '{', '}', '?', '\\\\', '/', '~', '|', '<', '>']\n",
    "\n",
    "# For printing the whole npy array\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_correct(query):\n",
    "    spell = SpellChecker()\n",
    "    misspelled = spell.unknown(query.split())\n",
    "    if misspelled:\n",
    "        for word in query.split():\n",
    "            if word in misspelled:\n",
    "                print(\"Correcting \" + word + \" to \" + spell.correction(word))\n",
    "                query = query.replace(word, spell.correction(word))\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemmed_token(token):\n",
    "    porter = nltk.PorterStemmer()\n",
    "    return porter.stem(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query_vector(query, vocabulary_keys, inverse_vocab_word_dict, term_document_frequency, N):\n",
    "    query_text = ''\n",
    "    for token in query:\n",
    "        query_text = query_text + ' ' + str(token)\n",
    "    query = query_text\n",
    "    query = query.lower()\n",
    "\n",
    "    # bonus heuristic\n",
    "    print(\"Running Spell Check\")\n",
    "    query = spell_correct(query)\n",
    "\n",
    "    query_vector = np.zeros(len(vocabulary_keys))\n",
    "    query = nltk.word_tokenize(query)\n",
    "\n",
    "    if(len(query) == 1 and query[0] not in inverse_vocab_word_dict):\n",
    "        print(\n",
    "            query[0] + \" is not found in vocabulary. Using most appropriate substitution using root word analysis! \")\n",
    "        stemmed_token = get_stemmed_token(query[0])\n",
    "\n",
    "        # Now the query contains all tokens as it is, only the ones that do not excist\n",
    "        # in the vocabulary are replaced by their stemmed root versions\n",
    "        stemmed_vocab = pickle.load(open(\"stemmed_vocab.pkl\", \"rb\"))\n",
    "        if(stemmed_token not in stemmed_vocab):\n",
    "            print(\"Could not replace, no search results found\")\n",
    "            exit(0)\n",
    "        fixed_token = stemmed_vocab[token][0]\n",
    "        print(\"Did you mean \" + fixed_token + \"? Press y for yes: \")\n",
    "        choice = input()\n",
    "        if choice != 'y':\n",
    "            print(\"No search results found\")\n",
    "            exit(0)\n",
    "        query[query.index(token)] = fixed_token\n",
    "        # Query has the wrong word replaced by the most common rooted word (with the same root as the wrong word)\n",
    "        print(\"Changed query: \" + ' '.join(query))\n",
    "        query_vector[inverse_vocab_word_dict[fixed_token]\n",
    "                     ] = query_vector[inverse_vocab_word_dict[fixed_token]] + 1\n",
    "    elif(len(query) >= 1):\n",
    "        for token in query:\n",
    "            if(token not in inverse_vocab_word_dict):\n",
    "                print(token + \" not found in vocabulary.\")\n",
    "                stemmed_token = get_stemmed_token(token)\n",
    "                stemmed_vocab = pickle.load(open(\"stemmed_vocab.pkl\", \"rb\"))\n",
    "                if(stemmed_token not in stemmed_vocab):\n",
    "                    continue\n",
    "                fixed_token = stemmed_vocab[stemmed_token][0]\n",
    "                print(\"Did you mean \" + fixed_token + \"? Press y for yes: \")\n",
    "                choice = input()\n",
    "                if choice != 'y':\n",
    "                    print(\"Skipping \" + token)\n",
    "                    continue\n",
    "                query[query.index(token)] = fixed_token\n",
    "                print(\"Changed query: \" + ' '.join(query))\n",
    "                query_vector[inverse_vocab_word_dict[fixed_token]\n",
    "                             ] = query_vector[inverse_vocab_word_dict[fixed_token]] + 1\n",
    "            else:\n",
    "                query_vector[inverse_vocab_word_dict[token]\n",
    "                             ] = query_vector[inverse_vocab_word_dict[token]] + 1\n",
    "\n",
    "    # processing log calculations (L)\n",
    "    for i in range(query_vector.shape[0]):\n",
    "        if(query_vector[i] > 0):\n",
    "            query_vector[i] = 1 + math.log(query_vector[i])\n",
    "\n",
    "    # processing term normalization (T)\n",
    "    for i in range(query_vector.shape[0]):\n",
    "        if(query_vector[i] == 0):\n",
    "            continue\n",
    "        query_vector[i] = query_vector[i] * \\\n",
    "            math.log(N/term_document_frequency[i])\n",
    "\n",
    "    # Cosine normalization (C)\n",
    "    temp_query_vector = np.copy(query_vector)\n",
    "    temp_query_vector = np.square(temp_query_vector)\n",
    "    temp_query_vector = np.sum(temp_query_vector)\n",
    "    temp_query_vector = np.sqrt(temp_query_vector)\n",
    "    query_vector = np.divide(query_vector, temp_query_vector)\n",
    "\n",
    "    return query, query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(query_vector, database_lnc):\n",
    "    scores = []\n",
    "    for id, document_vector in enumerate(database_lnc):\n",
    "        score = np.dot(query_vector, document_vector)\n",
    "        score = score/np.linalg.norm(query_vector)\n",
    "        score = score/np.linalg.norm(document_vector)\n",
    "        scores.append([id, score])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_weighting(scores, query, doc_titles):\n",
    "    title_weight = 0.1\n",
    "    trivial_words = [\"of\", \"and\", \"a\", \"the\", \"an\", \"is\"]\n",
    "    for doc_id, doc_title in enumerate(doc_titles):\n",
    "        count = 0\n",
    "        for word in query:\n",
    "            if word in doc_title:\n",
    "                if(word not in trivial_words):\n",
    "                    count = count + 1\n",
    "        scores[doc_id][1] = scores[doc_id][1] * (1+count*title_weight)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(query, database_lnc, vocabulary_keys, inverse_vocab_word_dict, term_document_frequency, N, doc_titles):\n",
    "    corrected_query, query_vector = process_query_vector(\n",
    "        query, vocabulary_keys, inverse_vocab_word_dict, term_document_frequency, N)\n",
    "    scores = calculate_score(query_vector, database_lnc)\n",
    "#     scores = title_weighting(scores, corrected_query , doc_titles)\n",
    "    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    print(\"Top 10 Scoring Documents are: \")\n",
    "    for ind in range(10):\n",
    "        if(scores[ind][1] == 0):\n",
    "            break\n",
    "        print(doc_titles[scores[ind][0]] + \" is at rank \" +\n",
    "              str(ind+1) + \" Score: \" + str(scores[ind][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_lnc = np.load(\"database_lnc.npy\")\n",
    "N = database_lnc.shape[0]\n",
    "vocabulary_dict = pickle.load(open(\"vocabulary_dict.pkl\", \"rb\"))\n",
    "\n",
    "vocabulary_keys = list(vocabulary_dict.keys())\n",
    "inverse_vocab_word_dict = {k: v for v, k in enumerate(vocabulary_keys)}\n",
    "term_document_frequency = np.count_nonzero(database_lnc, axis=0)\n",
    "doc_titles = pickle.load(open(\"doc_titles.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lonely widowers\n",
      "Running Spell Check\n",
      "widowers not found in vocabulary.\n",
      "Did you mean widow? Press y for yes: \n",
      "y\n",
      "Changed query: lonely widow\n",
      "Top 10 Scoring Documents are: \n",
      "\"Gheorghe Zamfir\" is at rank 1 Score: 0.048668874035561034\n",
      "\"Book of Ruth\" is at rank 2 Score: 0.03303445145059603\n",
      "\"Berry Berenson\" is at rank 3 Score: 0.030607013577806003\n",
      "\"Gaudy Night\" is at rank 4 Score: 0.029974901925442714\n",
      "\"Bram Stoker\" is at rank 5 Score: 0.025377098888839864\n",
      "\"George Orwell\" is at rank 6 Score: 0.022197025218706007\n",
      "\"Bestiary\" is at rank 7 Score: 0.021706467308098033\n",
      "\"Book of Lamentations\" is at rank 8 Score: 0.02132633262318457\n",
      "\"Bill Haley\" is at rank 9 Score: 0.019482329503754642\n",
      "\"Two Tribes\" is at rank 10 Score: 0.01914494259904734\n"
     ]
    }
   ],
   "source": [
    "query = input()\n",
    "query = query.split()\n",
    "scoring(query, database_lnc, vocabulary_keys, inverse_vocab_word_dict, term_document_frequency, N, doc_titles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
