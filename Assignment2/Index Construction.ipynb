{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "punctuations = ['.', ',', '!', '\\'', '\\\"',\n",
    "                '(', ')', '[', ']', '{', '}', '?', '\\\\', '/', '~', '|', '<', '>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary_freqdist(vocabulary):\n",
    "    vocabulary = nltk.FreqDist(vocabulary)\n",
    "    vocabulary = sorted(vocabulary.items(), key=lambda x: x[0])\n",
    "    vocabulary = dict(vocabulary)\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_the_vocab(vocabulary_dict):\n",
    "\n",
    "    vocabulary_keys = list(vocabulary_dict.keys())\n",
    "    vocabulary_values = list(vocabulary_dict.values())\n",
    "\n",
    "    porter = nltk.PorterStemmer()\n",
    "    stemmed_vocab = {}\n",
    "\n",
    "    for word_id, word in enumerate(vocabulary_keys):\n",
    "        if porter.stem(word) not in stemmed_vocab:\n",
    "            stemmed_vocab[porter.stem(word)] = (\n",
    "                word, vocabulary_values[word_id])\n",
    "        else:\n",
    "            if stemmed_vocab[porter.stem(word)][1] < vocabulary_values[word_id]:\n",
    "                stemmed_vocab[porter.stem(word)] = (\n",
    "                    word, vocabulary_values[word_id])\n",
    "\n",
    "    with open(\"stemmed_vocab.pkl\", \"wb\") as f:\n",
    "        pickle.dump(stemmed_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_database_vocabulary(filename):\n",
    "    with open(filename) as f:\n",
    "        text = f.read().replace('\\n', ' ')\n",
    "    database = []\n",
    "    vocabulary = []\n",
    "    doc_count = 0\n",
    "    i = 0\n",
    "    doc_titles = []\n",
    "\n",
    "    while(i < len(text)):\n",
    "\n",
    "        if(text[i] == '<' and text[i+1] == 'd' and text[i+2] == 'o' and text[i+3] == 'c'):\n",
    "\n",
    "            # keep finding the end of the doc begin tag\n",
    "            flag = 0\n",
    "            title = \"\"\n",
    "\n",
    "            while(text[i] != '>'):\n",
    "                i = i+1\n",
    "                if(text[i-6] == 't' and text[i-5] == 'i' and text[i-4] == 't' and text[i-3] == 'l' and text[i-2] == 'e' and text[i-1] == '='):\n",
    "                    flag = 1\n",
    "\n",
    "                if(flag == 1 and text[i] != '>'):\n",
    "                    title = title + str(text[i])\n",
    "\n",
    "            doc_titles.append(title)\n",
    "\n",
    "            i = i + 2\n",
    "\n",
    "            document = ''\n",
    "\n",
    "            while(not (text[i] == '<' and text[i+1] == '/' and text[i+2] == 'd' and text[i+3] == 'o')):\n",
    "                document = document + str(text[i])\n",
    "                i = i+1\n",
    "\n",
    "            while(text[i] != '>'):\n",
    "                i = i+1\n",
    "\n",
    "            doc_count = doc_count + 1\n",
    "\n",
    "            if(doc_count % 200 == 0):\n",
    "                print(\"Document \" + str(doc_count) + \" is being read...\")\n",
    "\n",
    "            # pre-processing\n",
    "            clean_document = bsoup(document, 'html.parser').get_text()\n",
    "            clean_document = clean_document.lower()\n",
    "            # tokens is a list of tokens\n",
    "            tokens = nltk.word_tokenize(clean_document)\n",
    "            tokens = [token for token in tokens if token not in punctuations]\n",
    "\n",
    "            for token in tokens:\n",
    "                vocabulary.append(token)\n",
    "\n",
    "            # saving the file / document wise write-back\n",
    "            # database is a list of list, as mentioned above\n",
    "            database.append(tokens)\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "    print(\"Done with the document reading...Database is ready \\n\")\n",
    "    print(\"There are total \" + str(doc_count) + \" documents!\")\n",
    "    print(\"There are total \" + str(len(doc_titles)) + \" titles!\")\n",
    "\n",
    "    # if(doc_count > 3):\n",
    "    #     break\n",
    "\n",
    "    # vocabulary is a dictionary of words/tokens and their corpus frequencies\n",
    "    vocabulary = build_vocabulary_freqdist(vocabulary)\n",
    "\n",
    "    print(\"Done with the Vocabulary building...Vocab is ready and saved !\\n\")\n",
    "\n",
    "    with open(\"vocabulary_dict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(vocabulary, f)\n",
    "\n",
    "    with open(\"doc_titles.pkl\", \"wb\") as f:\n",
    "        pickle.dump(doc_titles, f)\n",
    "\n",
    "    stem_the_vocab(vocabulary)\n",
    "\n",
    "    # print(vocabulary)\n",
    "    return database, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_documents_vector(database, vocabulary_words, inverse_vocab_word_dict):\n",
    "    documents_vector = np.zeros((len(database), len(vocabulary_words)))\n",
    "\n",
    "    # populate the documents_vector with the frequency of each vocabulary word for each document\n",
    "    for doc_id, doc in enumerate(database):\n",
    "        for token in doc:\n",
    "            documents_vector[doc_id][inverse_vocab_word_dict[token]\n",
    "                                     ] = documents_vector[doc_id][inverse_vocab_word_dict[token]] + 1\n",
    "    print(\"Done with the Documents Vector build... Saving it as numpy file \\n\")\n",
    "    np.save(\"documents_vector.npy\", documents_vector)\n",
    "    return documents_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents_vector(documents_vector):\n",
    "\n",
    "    for i in range(0, documents_vector.shape[0]):\n",
    "        for j in range(0, documents_vector.shape[1]):\n",
    "            if(documents_vector[i][j] > 0):\n",
    "                documents_vector[i][j] = 1 + math.log(documents_vector[i][j])\n",
    "\n",
    "    print(\"Done with the log calculation build... \\n\")\n",
    "\n",
    "    # calculation of cosine normalisation\n",
    "    temp_documents_vector = np.copy(documents_vector)\n",
    "    temp_documents_vector = np.square(temp_documents_vector)\n",
    "    temp_documents_vector = np.sum(temp_documents_vector, axis=1)\n",
    "    temp_documents_vector = np.sqrt(temp_documents_vector)\n",
    "    documents_vector = np.divide(\n",
    "        documents_vector, temp_documents_vector[:, None])\n",
    "\n",
    "    print(\"Done with the cosine normalization build... \\n\")\n",
    "    return documents_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_construction(filename):\n",
    "    database, vocabulary = build_database_vocabulary(filename)\n",
    "    # how many unique words\n",
    "    vocabulary_words = list(vocabulary.keys())\n",
    "    inverse_vocab_word_dict = {k: v for v, k in enumerate(vocabulary_words)}\n",
    "    documents_vector = build_documents_vector(\n",
    "        database, vocabulary_words, inverse_vocab_word_dict)\n",
    "    documents_vector = process_documents_vector(documents_vector)\n",
    "    np.save(\"database_lnc.npy\", documents_vector)\n",
    "    print(\"Saved! database_lnc.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 200 is being read...\n",
      "Document 400 is being read...\n",
      "Document 600 is being read...\n",
      "Document 800 is being read...\n",
      "Done with the document reading...Database is ready \n",
      "\n",
      "There are total 963 documents!\n",
      "There are total 963 titles!\n",
      "Done with the Vocabulary building...Vocab is ready and saved !\n",
      "\n",
      "Done with the Documents Vector build... Saving it as numpy file \n",
      "\n",
      "Done with the log calculation build... \n",
      "\n",
      "Done with the cosine normalization build... \n",
      "\n",
      "Saved! database_lnc.npy\n"
     ]
    }
   ],
   "source": [
    "index_construction(\"mega_wiki_corpus\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
